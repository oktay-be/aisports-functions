name: Deploy Result Merger Function

on:
  push:
    branches:
      - main
      - dev
    paths:
      - 'result_merger_function/**'
      - '.github/workflows/deploy-result-merger-function.yml'
  workflow_dispatch:

env:
  TRIGGER_LOCATION: 'us'  # Must match GCS bucket location
  FUNCTION_NAME: 'result-merger-function'

jobs:
  deploy:
    runs-on: ubuntu-latest
    
    permissions:
      contents: 'read'
      id-token: 'write'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          workload_identity_provider: 'projects/${{ vars.GCP_PROJECT_NUMBER }}/locations/global/workloadIdentityPools/${{ vars.WIF_POOL_ID }}/providers/${{ vars.WIF_PROVIDER_ID }}'
          service_account: ${{ vars.GCP_SERVICE_ACCOUNT }}
          export_environment_variables: true
      
      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: ${{ vars.GOOGLE_CLOUD_PROJECT }}
      
      - name: Deploy Result Merger Function
        uses: google-github-actions/deploy-cloud-functions@v3
        with:
          name: ${{ env.FUNCTION_NAME }}
          runtime: python312
          region: ${{ vars.REGION }}
          source_dir: ./result_merger_function
          entry_point: merge_results
          event_trigger_type: google.cloud.storage.object.v1.finalized
          event_trigger_location: ${{ env.TRIGGER_LOCATION }}
          event_trigger_filters: |
            bucket=${{ vars.GCS_BUCKET_NAME }}
          event_trigger_event_filters_path_pattern: news_data/batch_processing/**/stage1_extraction/results/*/predictions.jsonl
          memory: 1Gi
          cpu: 1
          service_timeout: 540
          max_instance_count: 10
          min_instance_count: 0
          max_instance_request_concurrency: 1
          environment_variables: |
            GOOGLE_CLOUD_PROJECT=${{ vars.GOOGLE_CLOUD_PROJECT }}
            GCS_BUCKET_NAME=${{ vars.GCS_BUCKET_NAME }}
            DEDUP_JOB_CREATED_TOPIC=${{ vars.DEDUP_JOB_CREATED_TOPIC }}
            VERTEX_AI_LOCATION=${{ vars.VERTEX_AI_LOCATION }}
            VERTEX_AI_MODEL=${{ vars.VERTEX_AI_MODEL }}
            NEWS_DATA_ROOT_PREFIX=${{ vars.NEWS_DATA_ROOT_PREFIX }}
            BATCH_PROCESSING_FOLDER=${{ vars.BATCH_PROCESSING_FOLDER }}
            BATCH_RESULTS_RAW_FOLDER=${{ vars.BATCH_RESULTS_RAW_FOLDER }}
            BATCH_RESULTS_MERGED_FOLDER=${{ vars.BATCH_RESULTS_MERGED_FOLDER }}
            DEDUP_RESULTS_FOLDER=${{ vars.DEDUP_RESULTS_FOLDER }}
          service_account: ${{ vars.GCP_SERVICE_ACCOUNT }}
      
      - name: Output function details
        run: |
          echo "Function deployed successfully!"
          echo "Name: ${{ env.FUNCTION_NAME }}"
          echo "Region: ${{ vars.REGION }}"
          echo "Trigger: GCS object finalization on bucket=${{ vars.GCS_BUCKET_NAME }}"
          echo "Pattern: news_data/batch_processing/**/stage1_extraction/results/*/predictions.jsonl"
          echo "Runtime: Python 3.12"
          echo "Memory: 1Gi"
          echo "Timeout: 540s"
          echo ""
          echo "The function will automatically trigger when Vertex AI batch jobs complete."
          echo "Published messages will be sent to: ${{ vars.DEDUP_JOB_CREATED_TOPIC }} topic"
